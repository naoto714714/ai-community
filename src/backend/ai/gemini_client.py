"""Gemini APIçµ±åˆãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«."""

import asyncio
import logging
import os
import re
import threading
from pathlib import Path

# å‹•çš„ã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚’é¿ã‘ã‚‹ãŸã‚ã®é™çš„ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
try:
    # ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã¨ã—ã¦å®Ÿè¡Œã•ã‚Œã‚‹å ´åˆ
    from .. import crud
    from .personality_manager import AIPersonality, get_personality_manager
except ImportError:
    # ç›´æ¥å®Ÿè¡Œã•ã‚Œã‚‹å ´åˆ
    import crud
    from ai.personality_manager import AIPersonality, get_personality_manager
from google import genai  # type: ignore
from google.genai import types  # type: ignore
from sqlalchemy.orm import Session

logger = logging.getLogger(__name__)


class GeminiAPIClient:
    """Gemini APIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ."""

    # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®ä¸€å…ƒç®¡ç†
    FALLBACK_MESSAGE = "é€šä¿¡ã«å¤±æ•—ã—ã¾ã—ãŸğŸ˜… ã‚‚ã†ä¸€åº¦è©¦ã—ã¦ã¿ã¦ãã ã•ã„ï¼"

    # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯äººæ ¼ï¼ˆã‚·ã‚¹ãƒ†ãƒ ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ç”¨ï¼‰
    FALLBACK_AI_NAME = "ã‚·ã‚¹ãƒ†ãƒ "
    FALLBACK_AI_ID = "ai_system"

    def __init__(self) -> None:
        """åˆæœŸåŒ–."""
        logger.info("GeminiAPIClientåˆæœŸåŒ–é–‹å§‹")
        self.api_key = os.getenv("GEMINI_API_KEY")
        if not self.api_key:
            logger.error("GEMINI_API_KEYç’°å¢ƒå¤‰æ•°ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
            raise ValueError("GEMINI_API_KEY environment variable is required")

        logger.info("GEMINI_API_KEYç¢ºèªæ¸ˆã¿")
        self.client = genai.Client(api_key=self.api_key)  # type: ignore
        logger.info("Gemini 2.5 Flash Preview 05-20ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆåˆæœŸåŒ–å®Œäº†")
        self.personality_manager = get_personality_manager()
        self._fallback_prompt: str | None = None
        self._load_fallback_prompt()

    def _load_fallback_prompt(self) -> None:
        """ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ç”¨ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’èª­ã¿è¾¼ã‚€."""
        # ç’°å¢ƒå¤‰æ•°ã‹ã‚‰åŸºæœ¬ãƒ‘ã‚¹ã‚’å–å¾—
        base_path = os.getenv("AI_COMMUNITY_BASE_PATH")
        if not base_path:
            # ã‚ˆã‚Šå®‰å…¨ãªãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼šç¾åœ¨ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‹ã‚‰æ¤œç´¢
            current = Path(__file__).parent
            while current != current.parent:
                prompt_dir = current / "prompts"
                if prompt_dir.exists():
                    base_path = str(current)
                    break
                current = current.parent

            if not base_path:
                logger.error("ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
                raise FileNotFoundError("ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã®ç‰¹å®šã«å¤±æ•—ã—ã¾ã—ãŸ")

        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ç”¨ã®ã‚·ãƒ³ãƒ—ãƒ«ãªãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’è¨­å®š
        self._fallback_prompt = """
ã‚ãªãŸã¯è¦ªã—ã¿ã‚„ã™ã„AIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚
ãƒ¦ãƒ¼ã‚¶ãƒ¼ã¨ã®ä¼šè©±ã‚’æ¥½ã—ã¿ã€å½¹ã«ç«‹ã¤æƒ…å ±ã‚’æä¾›ã—ã¾ã™ã€‚
"""
        logger.info("ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’è¨­å®š")
        return

        # ç©ºã®å‡¦ç†ï¼ˆä¸Šè¨˜ã§è¨­å®šæ¸ˆã¿ï¼‰

    def _select_random_personality(self) -> AIPersonality:
        """ãƒ©ãƒ³ãƒ€ãƒ ã«äººæ ¼ã‚’é¸æŠã—ã€ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚’ç®¡ç†."""
        try:
            # ãƒ©ãƒ³ãƒ€ãƒ ã«äººæ ¼ã‚’é¸æŠ
            personality = self.personality_manager.get_random_personality()
            if personality:
                logger.debug(f"ãƒ©ãƒ³ãƒ€ãƒ äººæ ¼é¸æŠ: {personality.name}")
                return personality
        except Exception as e:
            logger.error(f"äººæ ¼é¸æŠã‚¨ãƒ©ãƒ¼: {str(e)}")

        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯äººæ ¼ã‚’è¿”ã™
        logger.warning("ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯äººæ ¼ã‚’ä½¿ç”¨")
        return AIPersonality(
            file_name="fallback.md",
            name=self.FALLBACK_AI_NAME,
            prompt_content=self._fallback_prompt or "è¦ªã—ã¿ã‚„ã™ã„AIã§ã™ã€‚",
            user_id=self.FALLBACK_AI_ID,
        )

    def _format_conversation_history(self, messages: list) -> str:
        """éå»ã®ä¼šè©±å±¥æ­´ã‚’ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã™ã‚‹"""
        if not messages:
            return ""

        history_lines = ["===== éå»ã®ä¼šè©±å±¥æ­´ ====="]
        for msg in messages:
            # user_typeã‚’ä½¿ã£ã¦AIã‹ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‹ã‚’åˆ¤å®š
            if hasattr(msg, "user_type") and msg.user_type == "ai":
                # AIã®å ´åˆã¯ã€ã©ã®AIã‹ã‚’æ˜ç¢ºã«ã™ã‚‹
                history_lines.append(f"[AI:{msg.user_name}]: {msg.content}")
            else:
                # ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å ´åˆ
                history_lines.append(f"[ãƒ¦ãƒ¼ã‚¶ãƒ¼:{msg.user_name}]: {msg.content}")

        history_lines.append("")  # ç©ºè¡Œã‚’è¿½åŠ 
        return "\n".join(history_lines)

    async def _fetch_conversation_history(self, channel_id: str, db_session: Session) -> str:
        """ä¼šè©±å±¥æ­´ã‚’å–å¾—ã—ã¦ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã™ã‚‹"""
        try:
            recent_messages = crud.get_recent_channel_messages(db_session, channel_id, limit=30)
            logger.debug(f"ãƒ‡ãƒãƒƒã‚°: å–å¾—ã—ãŸãƒ¡ãƒƒã‚»ãƒ¼ã‚¸æ•°={len(recent_messages)}")
            for i, msg in enumerate(recent_messages[-5:]):  # æœ€æ–°5ä»¶ã‚’ãƒ­ã‚°å‡ºåŠ›
                logger.debug(f"ãƒ‡ãƒãƒƒã‚°: ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸{i}: user_id={msg.user_id}, content='{msg.content[:30]}...'")
            conversation_history = self._format_conversation_history(recent_messages)
            logger.info(f"éå»ã®ä¼šè©±å±¥æ­´ã‚’å–å¾—: {len(recent_messages)}ä»¶ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸")
            logger.debug(f"ãƒ‡ãƒãƒƒã‚°: conversation_history ã®é•·ã•={len(conversation_history)}")
            return conversation_history
        except Exception as e:
            logger.error(f"éå»ã®ä¼šè©±å±¥æ­´å–å¾—ã‚¨ãƒ©ãƒ¼: {str(e)}")
            import traceback

            logger.error(f"ã‚¨ãƒ©ãƒ¼è©³ç´°: {traceback.format_exc()}")
            return ""

    async def generate_response(
        self, user_message: str, channel_id: str | None = None, db_session: Session | None = None, max_retries: int = 5
    ) -> tuple[str, AIPersonality]:
        """
        ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã«å¯¾ã™ã‚‹å¿œç­”ã‚’ç”Ÿæˆã™ã‚‹.

        Args:
            user_message: ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‹ã‚‰ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
            channel_id: ãƒãƒ£ãƒ³ãƒãƒ«IDï¼ˆéå»ã®ä¼šè©±å±¥æ­´å–å¾—ç”¨ï¼‰
            db_session: ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚»ãƒƒã‚·ãƒ§ãƒ³
            max_retries: æœ€å¤§ãƒªãƒˆãƒ©ã‚¤å›æ•°

        Returns:
            tuple[AIã®å¿œç­”ãƒ†ã‚­ã‚¹ãƒˆ, é¸æŠã•ã‚ŒãŸäººæ ¼]
        """
        logger.info(f"Gemini APIå¿œç­”ç”Ÿæˆé–‹å§‹: user_message='{user_message[:50]}...' max_retries={max_retries}")

        # ãƒ©ãƒ³ãƒ€ãƒ ã«äººæ ¼ã‚’é¸æŠ
        personality = self._select_random_personality()
        logger.info(f"é¸æŠã•ã‚ŒãŸäººæ ¼: {personality.name}")

        # éå»ã®ä¼šè©±å±¥æ­´ã‚’å–å¾—
        conversation_history = ""
        logger.debug(f"ãƒ‡ãƒãƒƒã‚°: channel_id={channel_id}, db_session={db_session is not None}")
        if channel_id and db_session:
            conversation_history = await self._fetch_conversation_history(channel_id, db_session)
        else:
            logger.debug(
                f"ãƒ‡ãƒãƒƒã‚°: ä¼šè©±å±¥æ­´å–å¾—ã‚’ã‚¹ã‚­ãƒƒãƒ— - channel_id={channel_id}, db_session={db_session is not None}"
            )

        # æ–°ã—ã„APIã§ã¯system_instructionã§ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆè¨­å®š
        logger.debug(f"é¸æŠã•ã‚ŒãŸäººæ ¼ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆé•·: {len(personality.prompt_content)}")
        if conversation_history:
            logger.debug(f"ä¼šè©±å±¥æ­´é•·: {len(conversation_history)}")
            # ä¼šè©±å±¥æ­´ãŒã‚ã‚‹å ´åˆã¯æ–‡è„ˆã‚‚å«ã‚ã¦ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’æ§‹ç¯‰
            enhanced_message = f"{conversation_history}\n\nç¾åœ¨ã®è³ªå•: {user_message}"
        else:
            enhanced_message = user_message

        for attempt in range(max_retries):
            try:
                logger.info(f"Gemini APIå‘¼ã³å‡ºã—è©¦è¡Œ {attempt + 1}/{max_retries}")
                # éåŒæœŸã§Gemini APIã‚’å‘¼ã³å‡ºã—
                loop = asyncio.get_event_loop()
                response = await loop.run_in_executor(None, self._sync_generate, enhanced_message, personality)

                if response and hasattr(response, "text") and response.text:  # type: ignore
                    response_text = response.text.strip()  # type: ignore
                    logger.info(
                        f"Gemini APIå¿œç­”æˆåŠŸ: response_length={len(response_text)}, personality={personality.name}"
                    )
                    return response_text, personality

                logger.warning("Gemini APIã‹ã‚‰ç©ºã®å¿œç­”ã‚’å—ä¿¡")
                raise Exception("Empty response from Gemini API")

            except Exception as e:
                # ã‚ˆã‚Šå…·ä½“çš„ãªä¾‹å¤–å‡¦ç†
                error_type = type(e).__name__
                logger.error(f"Gemini APIå‘¼ã³å‡ºã—å¤±æ•— (è©¦è¡Œ {attempt + 1}/{max_retries}): {error_type}: {str(e)}")

                # ç‰¹å®šã®ã‚¨ãƒ©ãƒ¼ã‚¿ã‚¤ãƒ—ã«å¯¾ã™ã‚‹å‡¦ç†ãŒå¿…è¦ãªå ´åˆ
                # if isinstance(e, SpecificAPIError):
                #     # ç‰¹åˆ¥ãªå‡¦ç†

                if attempt == max_retries - 1:
                    # æœ€å¾Œã®ãƒªãƒˆãƒ©ã‚¤ã§ã‚‚å¤±æ•—ã—ãŸå ´åˆ
                    logger.error("Gemini API: å…¨ãƒªãƒˆãƒ©ã‚¤è©¦è¡ŒãŒå¤±æ•—ã€ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å¿œç­”ã‚’è¿”ã™")
                    return self.FALLBACK_MESSAGE, personality

                # æŒ‡æ•°ãƒãƒƒã‚¯ã‚ªãƒ•ã§ãƒªãƒˆãƒ©ã‚¤
                wait_time = 2**attempt
                logger.info(f"Gemini API: {wait_time}ç§’å¾Œã«ãƒªãƒˆãƒ©ã‚¤ã—ã¾ã™")
                await asyncio.sleep(wait_time)
                continue

        return self.FALLBACK_MESSAGE, personality

    def _sync_generate(self, user_message: str, personality) -> object | None:
        """
        åŒæœŸçš„ã«ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’ç”Ÿæˆã™ã‚‹ï¼ˆrun_in_executorç”¨ï¼‰.

        æ–°ã—ã„Google Genai APIã‚’ä½¿ç”¨ã—ã¦ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’ç”Ÿæˆã—ã¾ã™ã€‚
        """
        try:
            response = self.client.models.generate_content(
                model="gemini-2.5-flash-preview-05-20",
                contents=[user_message],
                config=types.GenerateContentConfig(  # type: ignore
                    system_instruction=personality.prompt_content,
                    temperature=0.9,
                    max_output_tokens=2000,
                    thinking_config=types.ThinkingConfig(thinking_budget=0),  # Disables thinking
                ),
            )
            return response
        except Exception as e:
            logger.error(f"Gemini APIå‘¼ã³å‡ºã—ã‚¨ãƒ©ãƒ¼: {str(e)}")
            return None

    def should_respond_to_message(self, message: str) -> bool:
        """
        ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã«å¿œç­”ã™ã¹ãã‹ã©ã†ã‹ã‚’åˆ¤å®šã™ã‚‹.

        Args:
            message: ãƒã‚§ãƒƒã‚¯ã™ã‚‹ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸

        Returns:
            å¿œç­”ã™ã¹ãå ´åˆTrue
        """
        # æ—¥æœ¬èªç’°å¢ƒã«å¯¾å¿œã—ãŸ@AIæ¤œå‡ºï¼ˆå…¨è§’ã‚¹ãƒšãƒ¼ã‚¹ã‚‚è€ƒæ…®ï¼‰
        # (?:^|[\sã€€]) - æ–‡é ­ã¾ãŸã¯åŠè§’ãƒ»å…¨è§’ç©ºç™½æ–‡å­—ã®å¾Œ
        # @ai - @aiã®ãƒªãƒ†ãƒ©ãƒ«ï¼ˆå¤§æ–‡å­—å°æ–‡å­—åŒºåˆ¥ãªã—ï¼‰
        # (?=[\sã€€]|$) - åŠè§’ãƒ»å…¨è§’ç©ºç™½æ–‡å­—ã¾ãŸã¯æ–‡æœ«ã®å‰
        pattern = r"(?:^|[\sã€€])@ai(?=[\sã€€]|$)"
        result = bool(re.search(pattern, message.lower()))
        logger.debug(f"@AIæ¤œå‡º: '{message[:50]}...' -> {result}")
        return result


# ã‚°ãƒ­ãƒ¼ãƒãƒ«ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
gemini_client: GeminiAPIClient | None = None
_lock = threading.Lock()


def get_gemini_client() -> GeminiAPIClient:
    """Gemini APIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®ã‚·ãƒ³ã‚°ãƒ«ãƒˆãƒ³ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’å–å¾—."""
    global gemini_client
    if gemini_client is None:
        with _lock:
            # ãƒ€ãƒ–ãƒ«ãƒã‚§ãƒƒã‚¯ãƒ­ãƒƒã‚­ãƒ³ã‚°
            if gemini_client is None:
                logger.info("æ–°ã—ã„GeminiAPIClientã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’ä½œæˆ")
                gemini_client = GeminiAPIClient()
    return gemini_client
